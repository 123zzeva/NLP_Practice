import streamlit as st
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from openai import OpenAI
from resume_docs import docs
import PyPDF2  # ç”¨äºPDFè¯»å–
import os
import re

# åˆå§‹åŒ–å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆDeepSeekï¼‰
client = OpenAI(
    api_key="a104bfaefd8152c7ce92eabf0b576cefaad307cd",
    base_url="https://api-w1ke45ednco9tce8.aistudio-app.com/v1"
)

# ç¼–ç çŸ¥è¯†åº“
st.write("æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“â€¦â€¦")
model = SentenceTransformer("all-MiniLM-L6-v2")
doc_embeddings = model.encode(docs)
index = faiss.IndexFlatL2(doc_embeddings.shape[1])
index.add(np.array(doc_embeddings))
st.write("å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")


# æ ¹æ®æé—®æ£€ç´¢
def retrieve_context(query, chunks, embeddings, index, top_k=5):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_k)
    context = "\n".join([chunks[i] for i in indices[0]])
    return context


# æŸ¥è¯¢ + è°ƒç”¨å¤§æ¨¡å‹
def generate_answer(query, context):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€å†ä¼˜åŒ–ä¸“å®¶ï¼ŒåŸºäºç”¨æˆ·ä¸Šä¼ çš„ç®€å†å›ç­”é—®é¢˜ã€‚"},
        {"role": "user", "content": f"ç®€å†å†…å®¹å¦‚ä¸‹ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"}
    ]

    response = client.chat.completions.create(
        model="deepseek-r1:1.5b",
        temperature=0.6,
        messages=messages
    )

    return response.choices[0].message.content

# æ‹†åˆ†æ€è€ƒéƒ¨åˆ†å’Œæ­£å¼å›ç­”éƒ¨åˆ†
def extract_thought_and_answer(answer_text):
    think_match = re.search(r"<think>(.*?)</think>", answer_text, re.DOTALL)
    thought = think_match.group(1).strip() if think_match else ""
    clean_answer = re.sub(r"<think>.*?</think>", "", answer_text, flags=re.DOTALL).strip()
    return thought, clean_answer

# ä»PDFæˆ–TXTä¸­æå–æ–‡å­—
def extract_text_from_file(uploaded_file):
    if uploaded_file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text
    elif uploaded_file.type == "text/plain":
        return uploaded_file.read().decode("utf-8")
    else:
        return "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼  PDF æˆ– TXT æ–‡ä»¶ã€‚"


# åˆ†å¥
def build_faiss_index(text):
    chunks = [chunk.strip() for chunk in text.split('\n') if chunk.strip()]
    embeddings = model.encode(chunks)

    # æ„å»ºå‘é‡ç´¢å¼•
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return index, chunks, embeddings


# ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½ï¼ˆå¤šè½®å¯¹è¯ï¼‰
chat_history = []
def generate_answer_with_memory(query, context, history):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†ä¼˜åŒ–åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡å’Œç”¨æˆ·é—®é¢˜è¿›è¡Œå›ç­”ã€‚"}
    ]

    # æ·»åŠ å†å²èŠå¤©è®°å½•
    messages.extend(history)

    # å½“å‰è½®é—®é¢˜ + ç®€å†ä¸Šä¸‹æ–‡
    messages.append({"role": "user", "content": f"ç®€å†å†…å®¹å¦‚ä¸‹ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"})

    # è¯·æ±‚å¤§æ¨¡å‹
    response = client.chat.completions.create(
        model="deepseek-r1:1.5b",
        temperature=0.6,
        messages=messages
    )

    # è§£æå›ç­”
    answer = response.choices[0].message.content

    # æŠŠè¿™è½®é—®ç­”åŠ å…¥å†å²
    history.append({"role": "user", "content": query})
    history.append({"role": "assistant", "content": answer})

    return answer, history


# Streamlit é¡µé¢
st.title("ğŸ“„ ç®€å†ä¼˜åŒ–é—®ç­”ç³»ç»Ÿ")
# st.write("è¯·è¾“å…¥ä½ çš„ç®€å†ä¼˜åŒ–ç›¸å…³é—®é¢˜ï¼Œæˆ‘å°†ä¸ºä½ æ£€ç´¢èµ„æ–™å¹¶ç”Ÿæˆä¸“ä¸šå»ºè®®~")
st.write("è¯·ä¸Šä¼ ä½ çš„ç®€å†ï¼Œæˆ‘å°†ä¸ºä½ æ£€ç´¢èµ„æ–™å¹¶ç”Ÿæˆä¸“ä¸šå»ºè®®~")

# ä¸Šä¼ ç®€å†æ–‡æ¡£
uploaded_file = st.file_uploader("ä¸Šä¼ ç®€å†æ–‡ä»¶ï¼ˆæ”¯æŒ PDF æˆ– TXTï¼‰", type=["pdf", "txt"])
resume_text = ""

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'clear_chat_flag' not in st.session_state:
    st.session_state.clear_chat_flag = False

# ç®€å†ä¸Šä¼ æˆåŠŸ
if uploaded_file:
    resume_text = extract_text_from_file(uploaded_file)
    st.success("ç®€å†ä¸Šä¼ æˆåŠŸï¼ä»¥ä¸‹æ˜¯ç®€å†å†…å®¹é¢„è§ˆï¼š")
    st.text_area("ç®€å†å†…å®¹", resume_text[:1000] + "..." if len(resume_text) > 1000 else resume_text, height=300)

qa_placeholder = st.empty()
with qa_placeholder.container():
    if st.session_state.get("clear_chat_flag", False):
        st.session_state.query = ""  # â¬…ï¸ åœ¨è¾“å…¥æ¡†æ¸²æŸ“å‰æ¸…ç©º
        st.session_state.clear_chat_flag = False
        st.stop()

    # é—®ç­”
    if uploaded_file and resume_text:
        index, chunks, embeddings = build_faiss_index(resume_text)

        query = st.text_input("ğŸ’¬ è¯·æé—®ï¼š", key="query", placeholder="æ¯”å¦‚ï¼šè¿™ä»½ç®€å†æœ‰ä»€ä¹ˆå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Ÿ")

        if query:
            context = retrieve_context(query, chunks, embeddings, index)
            answer, st.session_state.chat_history = generate_answer_with_memory(query, context, st.session_state.chat_history)
            thought, clean_answer = extract_thought_and_answer(answer)

            st.markdown("### ğŸ“š æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š")
            st.info(context)

            st.markdown("### ğŸ¤” æ€è€ƒ")
            st.info(thought if thought else "æ— ")
            st.markdown("### ğŸ¤– å›ç­”ï¼š")
            st.success(clean_answer)

            st.markdown("### ğŸ—‚ï¸ èŠå¤©è®°å½•ï¼š")
            for msg in st.session_state.chat_history:
                role = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
                st.markdown(f"**{role}ï¼š** {msg['content']}")

# æ¸…é™¤èŠå¤©è®°å½•
st.divider()
if st.button("ğŸ§¼ æ¸…é™¤èŠå¤©è®°å½•"):
    st.session_state.chat_history = []
    st.session_state.clear_chat_flag = True
    qa_placeholder.empty()
    st.rerun()